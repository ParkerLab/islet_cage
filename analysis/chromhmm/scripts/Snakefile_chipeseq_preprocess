import os
import pandas

DIR = config['results']
my_email = config['email']
BASE_DIR = os.getenv('BASE_DIR')
ENV_DIR = os.path.join(BASE_DIR, 'env')
SCRIPTS = os.path.join(BASE_DIR, "analysis/chromhmm/scripts/")
PARAMETERS = config['PARAMETERS']

workdir: DIR
         
onsuccess:
    shell("""mail -s "ChromHMM finished" {my_email} < {log}""")

onerror:
    print("An error occurred")
    shell("""mail -s "an error occurred in ChromHMM workflow" {my_email} < {log}""")

BASE_DATA_DIR = os.path.join(BASE_DIR, "data")

DATA = {
    'fastq' : "/lab/data/public/papers/pasquali_2014_K4me1/fastq/{fastq}.fastq.gz",
    'bwa_index': "/lab/data/reference/human/hg19/index/bwa/0.7.15/hg19",
    'adapter' : os.path.join(BASE_DATA_DIR, "islet_H3K4me1", "adapters.tsv"),
    'sample_info': os.path.join(BASE_DATA_DIR, "islet_H3K4me1", "sample_info.tsv"),
    'chromBed': os.path.join(BASE_DATA_DIR, "chromhmm", "chrom_virtual"),
    'phantom_env' : os.path.join(ENV_DIR, "phantom_peak.yaml"),
    'histone_bed_files': os.path.join(BASE_DATA_DIR, "histone_bed_files/{cell}.{mark}.hg19.merged.rmdup.bed.gz"),
    'hg19_lengths': os.path.join(BASE_DATA_DIR, "hg19_lengths.txt"),
    'states_2017': os.path.join(BASE_DATA_DIR, "pnas2017_states", "{{cell}}.{chromatinState}.bed"),
    'states_2017_types': glob_wildcards(os.path.join(BASE_DATA_DIR, "pnas2017_states", "Islets.{chromatinState}.bed"))[0]
    }

def get_dir(dirname):
    return os.path.join(DIR, dirname)

FASTQC_DIR = get_dir("fastqc")
LOG_DIR = get_dir("logs")
TRIM_DIR = get_dir("cutadapt")
BWA_DIR = get_dir("bwa")
MERGE_DIR = get_dir("merge_rmdup_q30")
PHANTOM_DIR = get_dir("phantompeak")
FIG_DIR = get_dir("figures")
BED_DIR = get_dir("bamToBed_files")


sdf = pandas.read_csv(DATA['sample_info'], sep='\t')
LIBRARIES = sdf['library'].drop_duplicates().tolist()

CHROM = list(range(1,23)) + ['X', 'Y']

def getFilename(mark):
    return sampleDF[sampleDF['marks'] == mark]['library'].drop_duplicates().tolist()
    
FASTQS = glob_wildcards(DATA['fastq'])[0]
fastq_names = {"H3K4me1" : FASTQS}
 
rule final_organize:
    """ Compare H3K4me1 data """
    input:
        fastqc = expand(os.path.join(TRIM_DIR, '{fastq}.trimmed.fastq.gz'),
                        fastq = FASTQS),
        bed = os.path.join(BASE_DATA_DIR, "histone_bed_files/Islets.H3K4me1.hg19.merged.rmdup.bed.gz"),

        
rule fastqc:
    """Run fastqc """
    input:
        DATA['fastq']
    output:
        os.path.join(FASTQC_DIR, '{fastq}_fastqc.zip')
    params:
        outdir = FASTQC_DIR
    log:
        os.path.join(LOG_DIR, 'fastqc.{fastq}.log')
    shell:
        'fastqc {input} -o {params.outdir} &> {log}'


def get_adapter_list():
    adapter_sequences = pandas.read_csv(DATA['adapter'], sep='\t')['sequence'].drop_duplicates().tolist()
    alist = ' '.join([f"-a {seq}" for seq in adapter_sequences ])
    return alist


rule cutadapt:
    """Trim adapter sequences """
    input:
        fastq = DATA['fastq']
    output:
        trimmed = os.path.join(TRIM_DIR, "{fastq}.trimmed.fastq.gz")
    params:
        adapter = get_adapter_list()
    shell:
        ' cutadapt {params.adapter} '
        ' -o {output.trimmed} '
        ' {input.fastq} '


rule map_single_end:
    """Get read 1 fastq to map """
    input:
        first = rules.cutadapt.output.trimmed,
        fasta = DATA['bwa_index']
    output:
        bam = os.path.join(BWA_DIR, "single_end", '{fastq}.bam')
    params:
        sort_tmp = os.path.join(
            BWA_DIR, "single_end", '{fastq}.sort.tmp'),
        rg = '\\t'.join(['@RG', 'ID:{}'.format('{fastq}'),
                         'LB:{}'.format('{fastq}')])
    threads:
        4
    resources:
        mem_mb = lambda wildcards, attempt: attempt * 15000 + 15000
    log:
        bwa = os.path.join(LOG_DIR, 'map.bwa.{fastq}.log'),
        samtools = os.path.join(LOG_DIR, 'map.samtools.{fastq}.log')
    shell:
        ' ionice -c2 -n7 bwa mem '
        ' -M -R "{params.rg}"'
        ' -t {threads} '
        ' {input.fasta} {input.first} 2> {log.bwa} | '
        ' samtools sort -m 1g -@ {threads} -O bam '
        ' -T {params.sort_tmp} -o {output} - 2> {log.samtools} '




rule merge_removeDuplicates:
    """ Merge replicates and remove duplicates using samtools (as was done previously) """
    input:
        bam = lambda wildcards: expand(rules.map_single_end.output.bam, fastq = sdf[sdf['library'] == wildcards.mark]['readgroup'].tolist()) 
    output:
        merged = temp(os.path.join(MERGE_DIR, "{mark}.merged.bam")),
        merge_rmdup = temp(os.path.join(MERGE_DIR, "{mark}.merged_rmdup.bam")),
        counts = os.path.join(MERGE_DIR, "{mark}.counts"),
        merge_rmdup_q30 = os.path.join(MERGE_DIR, "{mark}.merged_rmdup_q30.bam"),
    shell:
        r"""
        samtools merge  {output.merged} {input} ; 
        samtools view -c {output.merged} | awk '{{print "{wildcards.mark}\tafter_merge\t"$0"\n"}}' OFS='\t' > {output.counts} ;
        samtools rmdup -s {output.merged} {output.merge_rmdup} ; 
        samtools view -c {output.merge_rmdup} | awk '{{print "{wildcards.mark}\tafter_merge_dedup\t"$0"\n"}}' OFS='\t' >> {output.counts} ;
        samtools view -h -q 30 {output.merge_rmdup} > {output.merge_rmdup_q30} ; 
        samtools view -c {output.merge_rmdup_q30} | awk '{{print "{wildcards.mark}\tafter_merge_dedup_q30\t"$0"\n"}}' OFS='\t' >> {output.counts} ;
        """

        
rule phantomPeakQualTools:
    """ Computes quick but highly informative enrichment and quality measures and fragment lengths for ChIP-seq/DNase-seq/FAIRE-seq/MNase-seq data
    Summary
    This package computes quick but highly informative enrichment and quality measures for ChIP-seq/DNase-seq/FAIRE-seq/MNase-seq data. It can also be used to obtain robust estimates of the predominant fragment length or characteristic tag shift values in these assays.

    OPTIONAL ARGUMENTS -s=:: , strand shifts at which cross-correlation is evaluated, default=-500:5:1500 -speak=, user-defined cross-correlation peak strandshift -x=:, strand shifts to exclude (This is mainly to avoid region around phantom peak) default=10:(readlen+10) -p= , number of parallel processing nodes, default=0 -fdr= , false discovery rate threshold for peak calling -npeak=, threshold on number of peaks to call -tmpdir= , Temporary directory (if not specified R function tempdir() is used) -filtchr= , Pattern to use to remove tags that map to specific chromosomes e.g. _ will remove all tags that map to chromosomes with _ in their name

    OUTPUT ARGUMENTS -odir= name of output directory (If not set same as ChIP file directory is used) -savn= OR -savn NarrowPeak file name (fixed width peaks) -savr= OR -savr RegionPeak file name (variable width peaks with regions of enrichment around peak summits) -savd= OR -savd, save Rdata file -savp= OR -savp, save cross-correlation plot -out=, append peakshift/phantomPeak results to a file -rf, if plot or rdata or narrowPeak file exists replace it. If not used then the run is aborted if the plot or Rdata or narrowPeak file exists -clean, if used it will remove the original chip and control files after reading them in. CAUTION: Use only if the script calling run_spp.R is creating temporary files

    ===========================

    TYPICAL USAGE
    (1) Determine strand cross-correlation peak / predominant fragment length OR print out quality measures
    
    Rscript run_spp.R -c=<tagAlign/BAMfile> -savp -out=<outFile>
    -out= will create and/or append to a file named several important characteristics of the dataset. The file contains 11 tab delimited columns
    
    COL1: Filename: tagAlign/BAM filename COL2: numReads: effective sequencing depth i.e. total number of mapped reads in input file COL3: estFragLen: comma separated strand cross-correlation peak(s) in decreasing order of correlation. The top 3 local maxima locations that are within 90% of the maximum cross-correlation value are output. In almost all cases, the top (first) value in the list represents the predominant fragment length. If you want to keep only the top value simply run sed -r 's/,[^\t]+//g' > COL4: corr_estFragLen: comma separated strand cross-correlation value(s) in decreasing order (col2 follows the same order) COL5: phantomPeak: Read length/phantom peak strand shift COL6: corr_phantomPeak: Correlation value at phantom peak COL7: argmin_corr: strand shift at which cross-correlation is lowest COL8: min_corr: minimum value of cross-correlation COL9: Normalized strand cross-correlation coefficient (NSC) = COL4 / COL8 COL10: Relative strand cross-correlation coefficient (RSC) = (COL4 - COL8) / (COL6 - COL8) COL11: QualityTag: Quality tag based on thresholded RSC (codes: -2:veryLow,-1:Low,0:Medium,1:High,2:veryHigh)
"""
    input:
        rules.merge_removeDuplicates.output.merge_rmdup_q30
    output:
        stats = os.path.join(PHANTOM_DIR, "{mark}.scc"),
        plot = os.path.join(PHANTOM_DIR, "{mark}.merged_rmdup_q30.pdf") 
    params:
        script = os.path.join(SCRIPTS, "phantompeakqualtools/run_spp_nodups.R"),
        outDir = PHANTOM_DIR
    conda:
        DATA['phantom_env']
    shell:
        r"""
        Rscript {params.script} -c={input} -savp={output.plot} -out={output.stats} -odir={params.outDir}
        """

rule compile_phantomPeakQualTools:
    input:
        expand(rules.phantomPeakQualTools.output.stats, mark = LIBRARIES)
    output:
        os.path.join(PHANTOM_DIR, "allCorr.scc"),
    shell:
        r"""
        echo -e "name\tnumReads\testFragLen\tcorr_estFragLen\tphantomPeak\tcorr_phantomPeak\targmin_corr\tmin_corr\tNSC\tRSC\tQualityTag" | cat - {input} > {output}
        """
        
rule bamToBed:
    input:
        rmdup = rules.merge_removeDuplicates.output.merge_rmdup_q30,
        chromBed = DATA['chromBed']
    output:
        bed = os.path.join(BED_DIR, "{mark}.bed"),
    shell:
        """
        samtools view -b {input.rmdup} | 
        bamToBed -i stdin | 
        intersectBed -a - -b {input.chromBed} > {output.bed}
        """

rule organize:
    """Copy best library based on phantompeaks to the data folder """
    input:
        bed = expand(os.path.join(BED_DIR, "{mark}.bed"),
                     mark = PARAMETERS['chosen_k4me1_library'])
    output:
        bed = os.path.join(BASE_DATA_DIR, "histone_bed_files/Islets.H3K4me1.hg19.merged.rmdup.bed.gz"),
    shell:
        " gzip -c {input.bed} > {output.bed}"
        
